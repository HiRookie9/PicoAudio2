defaults:
  - basic
  - model: diffusion
  - data@data_dict: example
  - loss@loss_fn: identity
  - _self_

exp_dir: experiments/base

sample_rate: 24000

downsampling_ratio : 480

train_dataloader:
  _target_: torchdata.stateful_dataloader.StatefulDataLoader
  dataset:
    _target_: data_module.dataset.AudioGenConcatDataset
    datasets: ${data_dict.train_data_list}
  batch_size: 24 # per device batch size
  shuffle: True
  num_workers: 4
  collate_fn:
    _target_: data_module.collate_function.PaddingCollate
    pad_keys: ["waveform"]

val_dataloader:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: data_module.dataset.AudioGenConcatDataset
    datasets: ${data_dict.val_data_list}
  batch_size: 24 # per device batch size
  shuffle: False
  num_workers: 4
  collate_fn:
    _target_: data_module.collate_function.PaddingCollate
    pad_keys: ["waveform"]
  
trainer:
  _target_: audio_generation_trainer.AudioGenerationTrainer
  project_dir: ${exp_dir}
  logging_file: ${logging_file}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  epochs: ${epochs}
  epoch_length: ${epoch_length}
  save_every_n_epochs: ${save_every_n_epochs}
  save_last_k: 1
  early_stop: 100
  wandb_config:
    _target_: trainer.WandbConfig
    project: pico2
    save_dir: ${exp_dir}
    name: pico2_example
  metric_monitor:
    _target_: trainer.MetricMonitor
    metric_name: loss
    mode: min